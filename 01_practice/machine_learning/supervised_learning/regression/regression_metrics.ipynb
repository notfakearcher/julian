{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jra_regression_metrics(y_true, y_pred, p):\n",
    "    \n",
    "  # bias - inability of model causing error between predicted and actual values [-inf, inf]\n",
    "  def jra_ml_bias(y_true, y_pred):\n",
    "    x = (np.mean(y_pred) - y_true).sum()\n",
    "    return(x)\n",
    "  \n",
    "  # r squared - goodness of fit; amount of variability explained [0, 1]\n",
    "  def jra_ml_rsquared(y_true, y_pred):\n",
    "    a = np.power((y_pred - y_true), 2).sum()\n",
    "    b = np.power((np.mean(y_true) - y_true), 2).sum()\n",
    "    x = 1 - (a / b)\n",
    "    return(x)\n",
    "  \n",
    "  # adjusted r squared - goodness of fit; amount of variability explained while penalizing for excess variabels [0, 1]\n",
    "  def jra_ml_arsquared(y_true, y_pred, p):\n",
    "    a = np.power((y_pred - y_true), 2).sum()\n",
    "    b = np.power((np.mean(y_true) - y_true), 2).sum()\n",
    "    r2 = 1 - (a / b)\n",
    "    n = len(y_true)\n",
    "    c = (1 - r2) * (n - 1)\n",
    "    d = n - p - 1\n",
    "    x =  1 - (c / d)\n",
    "    return(x)\n",
    "  \n",
    "  # mean squared error - goodness of fit; how close predicted matches true values; gives more weight to larger errors [0, Inf] [0, Inf]\n",
    "  def jra_ml_mse(y_true, y_pred):\n",
    "    a = y_pred - y_true\n",
    "    x = np.power(a, 2).mean()\n",
    "    return(x)\n",
    "  \n",
    "  # root mean squared error - like mse but smaller values and units match original [0, Inf]\n",
    "  def jra_ml_rmse(y_true, y_pred):\n",
    "    a = y_pred - y_true\n",
    "    b = np.power(a, 2).mean()\n",
    "    x = np.sqrt(b)\n",
    "    return(x)\n",
    "  \n",
    "  # mean squared log error - like mse but offsets large deviations between predicted values and true values [0, Inf]\n",
    "  def jra_ml_msle(y_true, y_pred):\n",
    "    a = np.log((y_pred + 1)) - np.log((y_true + 1))\n",
    "    x = np.power(a, 2).mean()\n",
    "    return(x)\n",
    "  \n",
    "  # root mean squared log error - like rmse but used when predicted values largely deviate true values [0, Inf]\n",
    "  def jra_ml_rmsle(y_true, y_pred):\n",
    "    a = np.log((y_pred + 1)) - np.log((y_true + 1))\n",
    "    b = np.power(a, 2).mean()\n",
    "    x = np.sqrt(b)\n",
    "    return(x)\n",
    "  \n",
    "  # relative root mean squared error - relative/ratio error of model compared to naive/average model [0, Inf]\n",
    "  def jra_ml_rrmse(y_true, y_pred):\n",
    "    a = y_pred - y_true\n",
    "    b = np.power(a, 2).mean()\n",
    "    c = np.sqrt(b)\n",
    "    d = y_pred - np.mean(y_true)\n",
    "    e = np.power(d, 2).mean()\n",
    "    f = np.sqrt(e)\n",
    "    x = c / f\n",
    "    return(x)\n",
    "  \n",
    "  # normalized mean squared error - relative/ratio error of model normalized for cross-model comparisons [0, Inf]\n",
    "  def jra_ml_nmse(y_true, y_pred):\n",
    "    a = y_pred - y_true\n",
    "    b = np.power(a, 2).mean()\n",
    "    x = b / np.var(y_true)\n",
    "    return(x)\n",
    "  \n",
    "  # normalized root mean squared error - like nmse but smaller values and units match original [0, Inf] [0, Inf]\n",
    "  def jra_ml_nrmse(y_true, y_pred):\n",
    "    a = y_pred - y_true\n",
    "    b = np.power(a, 2).mean()\n",
    "    x = np.sqrt(b) / np.std(y_true)\n",
    "    return(x)\n",
    "  \n",
    "  # mean absolute error - goodness of fit; how close predicted matches true values not taking direcion into account; gives equal weight to errors [0, Inf]\n",
    "  def jra_ml_mae(y_true, y_pred):\n",
    "    a = np.abs(y_pred - y_true)\n",
    "    x = a.mean()\n",
    "    return(x)\n",
    "  \n",
    "  # mean absolute percent error - goodness of fit; relative magnitue of error produced on average [0, Inf]\n",
    "  def jra_ml_mape(y_true, y_pred):\n",
    "    a = np.abs(y_pred - y_true)\n",
    "    b = a / np.abs(y_true) \n",
    "    x = b.mean() * 100\n",
    "    return(x)\n",
    "  \n",
    "  # symmetric mean absolute percent error - goodness of fit; normalized relative magnitue of error produced on average [0, 200]\n",
    "  def jra_ml_smape(y_true, y_pred):\n",
    "    a = np.abs(y_pred - y_true) * 2\n",
    "    b = np.abs(y_pred) + np.abs(y_true)\n",
    "    x = (a / b).mean() * 100\n",
    "    return(x)\n",
    "  \n",
    "  # mean relative absolute error - relative/ratio absolute error of model compared to naive/average model [0, Inf]\n",
    "  def jra_ml_mrae(y_true, y_pred):\n",
    "    a = np.abs(y_pred - y_true)\n",
    "    b = y_true - np.mean(y_true)\n",
    "    c = np.abs(b)\n",
    "    x = (a.mean() / c.mean())\n",
    "    return(x)\n",
    "  \n",
    "  # median absolute error - goodness of fit; median differences between predicted and true values not taking direcion into account; gives equal weight to errors [0, Inf]\n",
    "  def jra_ml_mdae(y_true, y_pred):\n",
    "    a = np.abs(y_pred - y_true)\n",
    "    x = a.median()\n",
    "    return(x)\n",
    "  \n",
    "  # median relative absolute error - median relative/ratio absolute error of model compared to naive/average model [0, Inf]\n",
    "  def jra_ml_mdrae(y_true, y_pred):\n",
    "    a = np.abs(y_pred - y_true)\n",
    "    b = y_true - np.mean(y_true)\n",
    "    c = np.abs(b)\n",
    "    x = (a.median() / c.median())\n",
    "    return(x)\n",
    "  \n",
    "  df_metrics = pd.DataFrame({\n",
    "      'bias': jra_ml_bias(y_true, y_pred),\n",
    "      'rsquared': jra_ml_rsquared(y_true, y_pred),\n",
    "      'arsquared': jra_ml_arsquared(y_true, y_pred, p),\n",
    "      'mse': jra_ml_mse(y_true, y_pred),\n",
    "      'rmse': jra_ml_rmse(y_true, y_pred),\n",
    "      'rmsle': jra_ml_rmsle(y_true, y_pred),\n",
    "      'rrmse': jra_ml_rrmse(y_true, y_pred),\n",
    "      'nrmse': jra_ml_nrmse(y_true, y_pred), \n",
    "      'mae': jra_ml_mae(y_true, y_pred),\n",
    "      'mape': jra_ml_mape(y_true, y_pred),\n",
    "      'smape': jra_ml_smape(y_true, y_pred),\n",
    "      'mrae': jra_ml_mrae(y_true, y_pred),\n",
    "      'mdae': jra_ml_mdae(y_true, y_pred),\n",
    "      'mdrae': jra_ml_mdrae(y_true, y_pred),\n",
    "    }, index = ['Metrics'])\n",
    "  df_metrics.columns = [x.upper() for x in df_metrics.columns]\n",
    "  df_metrics = df_metrics.apply(lambda x: round(x, 4)).T\n",
    "  \n",
    "  return(df_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonupskill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
