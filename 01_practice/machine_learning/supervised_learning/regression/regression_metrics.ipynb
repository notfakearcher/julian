{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias - inability of model causing error between predicted and actual values [-inf, inf]\n",
    "def jra_ml_bias(y_true, y_pred):\n",
    "  x = (np.mean(y_pred) - y_true).sum()\n",
    "  return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r squared - goodness of fit; amount of variability explained [0, 1]\n",
    "def jra_ml_rsquared(y_true, y_pred):\n",
    "  a = np.power((y_pred - y_true), 2).sum()\n",
    "  b = np.power((np.mean(y_true) - y_true), 2).sum()\n",
    "  x = 1 - (a / b)\n",
    "  return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error - goodness of fit; how close predicted matches true values; gives more weight to larger errors [0, Inf] [0, Inf]\n",
    "def jra_ml_mse(y_true, y_pred):\n",
    "  a = y_pred - y_true\n",
    "  x = np.power(a, 2).mean()\n",
    "  return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error - like mse but smaller values and units match original [0, Inf]\n",
    "def jra_ml_rmse(y_true, y_pred):\n",
    "  a = y_pred - y_true\n",
    "  b = np.power(a, 2).mean()\n",
    "  x = np.sqrt(b)\n",
    "  return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared log error - like mse but offsets large deviations between predicted values and true values [0, Inf]\n",
    "def jra_ml_msle(y_true, y_pred):\n",
    "  a = np.log((y_pred + 1)) - np.log((y_true + 1))\n",
    "  x = np.power(a, 2).mean()\n",
    "  return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared log error - like rmse but used when predicted values largely deviate true values [0, Inf]\n",
    "def jra_ml_rmsle(y_true, y_pred):\n",
    "  a = np.log((y_pred + 1)) - np.log((y_true + 1))\n",
    "  b = np.power(a, 2).mean()\n",
    "  x = np.sqrt(b)\n",
    "  return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative root mean squared error - relative/ratio error of model compared to naive/average model [0, Inf]\n",
    "def jra_ml_rrmse(y_true, y_pred):\n",
    "  a = y_pred - y_true\n",
    "  b = np.power(a, 2).mean()\n",
    "  c = np.sqrt(b)\n",
    "  \n",
    "  d = y_pred - np.mean(y_true)\n",
    "  e = np.power(d, 2).mean()\n",
    "  f = np.sqrt(e)\n",
    "  \n",
    "  x = c / f\n",
    "  return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized mean squared error - relative/ratio error of model normalized for cross-model comparisons [0, Inf]\n",
    "def jra_ml_nmse(y_true, y_pred):\n",
    "  a = y_pred - y_true\n",
    "  b = np.power(a, 2).mean()\n",
    "  x = b / np.var(y_true)\n",
    "  return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized root mean squared error - like nmse but smaller values and units match original [0, Inf] [0, Inf]\n",
    "def jra_ml_nrmse(y_true, y_pred):\n",
    "  a = y_pred - y_true\n",
    "  b = np.power(a, 2).mean()\n",
    "  x = np.sqrt(b) / np.std(y_true)\n",
    "  return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean absolute error - goodness of fit; how close predicted matches true values not taking direcion into account; gives equal weight to errors [0, Inf]\n",
    "def jra_ml_mae(y_true, y_pred):\n",
    "  a = np.abs(y_pred - y_true)\n",
    "  x = a.mean()\n",
    "  return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean absolute percent error - goodness of fit; relative magnitue of error produced on average [0, Inf]\n",
    "def jra_ml_mape(y_true, y_pred):\n",
    "  a = np.abs(y_pred - y_true)\n",
    "  b = a / np.abs(y_true)\n",
    "  x = b.mean()\n",
    "  return(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonupskill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
